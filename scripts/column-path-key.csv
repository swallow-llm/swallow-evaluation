column,path,key,max_score
XLSUM_ja_1shot,ja/xlsum/xlsum_1shot_allcases/score_xlsum.json,results.xlsum_ja.rouge2,100
MATH (mgsm_ja),ja/mgsm/math_4shot_allcases/score_math.json,results.mgsm.acc,1
wmt20_en_ja_bleu,ja/wmt20_en_ja/wmt20_en_ja_4shot_allcases/score_wmt20_en_ja.json,results.wmt20-en-ja.bleu,100
wmt20_ja_en_bleu,ja/wmt20_ja_en/wmt20_ja_en_4shot_allcases/score_wmt20_ja_en.json,results.wmt20-ja-en.bleu,100
MC,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,x,1
NLI,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,x,1
QA,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,x,1
RC,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,x,1
jamp (NLI),ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jamp_exact_match,1
janli (NLI),ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.janli_exact_match,1
jcommonsenseqa,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jcommonsenseqa_exact_match,1
jemhopqa,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jemhopqa_char_f1,1
jnli,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jnli_exact_match,1
jsem,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jsem_exact_match,1
jsick (NLI),ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jsick_exact_match,1
jsquad,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jsquad_char_f1,1
jsts_pearson,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jsts_pearson,1
jsts_spearman,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.jsts_spearman,1
niilc,ja/llmjp/4shot_-1cases/llm_jp_eval_general.json,scores.niilc_char_f1,1
jmmlu,ja/llmjp/5shot_-1cases/llm_jp_eval_jmmlu_per_task.json,results.jmmlu,1
jmmlu_social_sciences,ja/llmjp/5shot_-1cases/llm_jp_eval_jmmlu_per_task.json,results.jmmlu_social_sciences,1
jmmlu_humanities,ja/llmjp/5shot_-1cases/llm_jp_eval_jmmlu_per_task.json,results.jmmlu_humanities,1
jmmlu_stem,ja/llmjp/5shot_-1cases/llm_jp_eval_jmmlu_per_task.json,results.jmmlu_stem,1
jmmlu_other,ja/llmjp/5shot_-1cases/llm_jp_eval_jmmlu_per_task.json,results.jmmlu_other,1
jhumaneval@1,ja/humaneval/metrics.json,humaneval.pass@1,1
jhumaneval@10,ja/humaneval/metrics.json,humaneval.pass@10,1
jhumaneval_answer@10,ja/humaneval/metrics.json,humaneval.answer@10,1
MT-Bench (ALL),ja/ja_mt_bench/judge.json,MODEL_NAME.overall.average.score,10
writing,ja/ja_mt_bench/judge.json,MODEL_NAME.Writing.average.score,10
roleplay,ja/ja_mt_bench/judge.json,MODEL_NAME.Roleplay.average.score,10
reasoning,ja/ja_mt_bench/judge.json,MODEL_NAME.Reasoning.average.score,10
math,ja/ja_mt_bench/judge.json,MODEL_NAME.Math.average.score,10
coding,ja/ja_mt_bench/judge.json,MODEL_NAME.Coding.average.score,10
extraction,ja/ja_mt_bench/judge.json,MODEL_NAME.Extraction.average.score,10
stem,ja/ja_mt_bench/judge.json,MODEL_NAME.STEM.average.score,10
humanities,ja/ja_mt_bench/judge.json,MODEL_NAME.Humanities.average.score,10
gsm8k,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.gsm8k.exact_match,strict-match",1
squad2,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.squadv2.exact,none",100
triviaqa,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.triviaqa.exact_match,remove_whitespace",1
hellaswag,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.hellaswag.acc,none",1
openbookqa,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.openbookqa.acc,none",1
xwinograd_en,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.xwinograd_en.acc,none",1
bbh_cot,en/harness_en/alltasks_3shot_allcases/bbh_cot/results.json,"results.bbh_cot_fewshot.exact_match,get-answer",1
mmlu,en/harness_en/alltasks_5shot_allcases/mmlu/results.json,"groups.mmlu.acc,none",1
mmlu_social_sciences,en/harness_en/alltasks_5shot_allcases/mmlu/results.json,"groups.mmlu_social_sciences.acc,none",1
mmlu_humanities,en/harness_en/alltasks_5shot_allcases/mmlu/results.json,"groups.mmlu_humanities.acc,none",1
mmlu_stem,en/harness_en/alltasks_5shot_allcases/mmlu/results.json,"groups.mmlu_stem.acc,none",1
mmlu_other,en/harness_en/alltasks_5shot_allcases/mmlu/results.json,"groups.mmlu_other.acc,none",1
humaneval@1,en/humaneval/metrics.json,humaneval.pass@1,1
humaneval@10,en/humaneval/metrics.json,humaneval.pass@10,1
humaneval_answer@10,en/humaneval/metrics.json,humaneval.answer@10,1
jhumaneval-unstripped@1,ja/humaneval-unstripped/metrics.json,humaneval.pass@1,1
jhumaneval-unstripped@10,ja/humaneval-unstripped/metrics.json,humaneval.pass@10,1
jhumaneval-unstripped_answer@10,ja/humaneval-unstripped/metrics.json,humaneval.answer@10,1
humaneval-unstripped@1,en/humaneval-unstripped/metrics.json,humaneval.pass@1,1
humaneval-unstripped@10,en/humaneval-unstripped/metrics.json,humaneval.pass@10,1
humaneval-unstripped_answer@10,en/humaneval-unstripped/metrics.json,humaneval.answer@10,1
MT-Bench (ALL)_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.overall.average.japanese_char_ratio,1
writing_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Writing.average.japanese_char_ratio,1
roleplay_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Roleplay.average.japanese_char_ratio,1
reasoning_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Reasoning.average.japanese_char_ratio,1
math_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Math.average.japanese_char_ratio,1
coding_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Coding.average.japanese_char_ratio,1
extraction_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Extraction.average.japanese_char_ratio,1
stem_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.STEM.average.japanese_char_ratio,1
humanities_ja_char_ratio,ja/ja_mt_bench/judge.json,MODEL_NAME.Humanities.average.japanese_char_ratio,1
mbpp_ja@1,ja/mbpp/metrics.json,mbpp.pass@1,1
mbpp_ja@10,ja/mbpp/metrics.json,mbpp.pass@10,1
mbpp_ja_answer@10,ja/mbpp/metrics.json,mbpp.answer@10,1
mbpp@1,en/mbpp/metrics.json,mbpp.pass@1,1
mbpp@10,en/mbpp/metrics.json,mbpp.pass@10,1
mbpp_answer@10,en/mbpp/metrics.json,mbpp.answer@10,1
mt_bench_1st_turn,ja/ja_mt_bench/judge.json,MODEL_NAME.overall.first_turn.score,10
mt_bench_2st_turn,ja/ja_mt_bench/judge.json,MODEL_NAME.overall.second_turn.score,10
minerva_math,en/harness_en/alltasks_4shot_allcases/minerva_math/results.json,"groups.minerva_math.exact_match,none",1
gpqa_main_meta_llama3_cot_zeroshot,en/harness_en/alltasks_0shot_allcases/gpqa_main_cot_zeroshot_meta_llama3_wo_chat/results.json,"results.gpqa_main_cot_zeroshot_meta_llama3_wo_chat.exact_match,strict-match",1
squad2_best_exact,en/harness_en/alltasks_4shot_allcases/general/results.json,"results.squadv2.best_exact,none",100
math_500,en/harness_en/alltasks_4shot_allcases/math_500/results.json,"results.math_500.exact_match,none",1
